import tensorflow as tf

#
# Generated by ChatGPT
#
def specificity(y_true, y_pred):
    neg_y_true = 1 - y_true
    neg_y_pred = 1 - tf.round(y_pred)
    true_negatives = tf.reduce_sum(tf.cast(neg_y_true * neg_y_pred, tf.float32))
    possible_negatives = tf.reduce_sum(tf.cast(neg_y_true, tf.float32))
    specificity = true_negatives / (possible_negatives + tf.keras.backend.epsilon())
    return specificity
    
def specificity_two_class(y_true, y_pred):
    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)
    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)
    
    # Convert from one-hot encoding to binary labels
    y_true_binary = tf.cast(y_true[:, 0], tf.float32)  # [1, 0] -> 1, [0, 1] -> 0
    y_pred_binary = tf.cast(tf.round(y_pred[:, 0]), tf.float32)  # Same conversion for predictions
    
     
    return specificity(y_true_binary, y_pred_binary)

def specificity_multilabel(y_true, y_pred):
    return specificity(tf.reshape(y_true, [-1]), tf.reshape(y_pred, [-1]))
    
# def specificity_multilabel(y_true, y_pred):
#     # Assuming y_true and y_pred are 2D tensors of shape [batch_size, num_labels]
#     num_labels = tf.shape(y_true)[1]
    
#     # Calculate specificity for each label
#     specificities = []
#     for i in range(num_labels):
#         specificities.append(specificity(y_true[:, i], y_pred[:, i]))
    
#     # Convert list to tensor
#     specificities = tf.stack(specificities)
    
#     # Compute average specificity across labels (macro-average)
#     avg_specificity = tf.reduce_mean(specificities)
    
#     return avg_specificity
#
# Generated by ChatGPT
#

def f1_score(y_true, y_pred):
    # Convert predictions to binary values
    y_pred_binary = tf.round(y_pred)
    
    # True Positives, False Positives, and False Negatives
    tp = tf.reduce_sum(tf.cast(y_true * y_pred_binary, tf.float32))
    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred_binary, tf.float32))
    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred_binary), tf.float32))
    
    # Calculate Precision and Recall
    precision = tp / (tp + fp + tf.keras.backend.epsilon())
    recall = tp / (tp + fn + tf.keras.backend.epsilon())
    
    # Calculate F1 score
    f1_score = 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))
    
    return f1_score

def weighted_f1_score(y_true, y_pred):
    # Convert predictions to binary values
    y_pred_binary = tf.round(y_pred)
    
    # True Positives, False Positives, and False Negatives
    tp = tf.reduce_sum(tf.cast(y_true * y_pred_binary, tf.float32))
    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred_binary, tf.float32))
    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred_binary), tf.float32))
    
    # Calculate Precision and Recall
    precision = tp / (tp + fp + tf.keras.backend.epsilon())
    recall = tp / (tp + fn + tf.keras.backend.epsilon())
    
    # Calculate F1 score
    f1_score = 1.5 * ((precision * recall) / (0.5*precision + recall + tf.keras.backend.epsilon()))
    
    return f1_score

def weighted_f1_score_multilabeled(y_true, y_pred):
    return weighted_f1_score(tf.reshape(y_true, [-1]), tf.reshape(y_pred, [-1]))

    
    # # Assuming y_true and y_pred are 2D tensors of shape [batch_size, num_labels]
    # num_labels = tf.shape(y_true)[1]
    
    # # Calculate specificity for each label
    # f1_scores = []
    # for i in range(num_labels):
    #     label_true = y_true[:, i]
    #     label_pred = y_pred[:, i]
    #     f1 = weighted_f1_score(label_true, label_pred)
    #     f1_scores.append(f1)
    
    # # Convert list to tensor
    # f1_scores = tf.stack(f1_scores)
    
    # # Compute average specificity across labels (macro-average)
    # avg_f1_scores = tf.reduce_mean(f1_scores)
    
    # return avg_f1_scores